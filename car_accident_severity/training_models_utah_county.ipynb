{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_data import clean_data\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a constant that will be used for anything\n",
    "# that needs uses randomness, so that our tests stay\n",
    "# consistent and repeatable\n",
    "RANDOM_STATE = 0\n",
    "METRICS = ['accuracy', 'f1_macro', 'f1_micro']\n",
    "NUM_FEAT_IMPORTANCES = 10\n",
    "DOWNSAMPLE_LABEL = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data and Format it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_and_format(filepath: str = \"./data/utah_county_accidents.csv\", \n",
    "                         test_perc: float = .3,\n",
    "                         index_column = 'ID',\n",
    "                         label_column = 'Severity',\n",
    "                         columns_to_drop: list = ['Unnamed: 0', 'End_Time', \n",
    "                                                  'County', 'State', 'City', \n",
    "                                                  'Country', 'Timezone', \n",
    "                                                  'Airport_Code', 'Street', \n",
    "                                                  'Zipcode', 'Source', \n",
    "                                                  'Description', 'Weather_Timestamp', \n",
    "                                                  'Wind_Direction', 'Nautical_Twilight', \n",
    "                                                  'Astronomical_Twilight'],\n",
    "                         dummy_columns: list = ['Month', 'Day', 'Civil_Twilight', 'Sunrise_Sunset'],\n",
    "                         rand_state=RANDOM_STATE,\n",
    "                         stratify=True,\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    -------\n",
    "    filepath (str): the path to the data\n",
    "    test_perc (float): the percent of test data\n",
    "    rand_state (bool): Should stay true (Don't change unless the group all agrees)\n",
    "    stratify (bool): Should be True (that way each class is equally represented\n",
    "                     in the test and train set)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train (pd.DataFrame): the training data\n",
    "    y_train (pd.DataFrame): the training labels\n",
    "    X_test (pd.DataFrame): the testing data\n",
    "    y_test (pd.DataFrame): the testing labels\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath, index_col=index_column)\n",
    "\n",
    "    df = clean_data(df, to_drop=columns_to_drop)\n",
    "\n",
    "    df = pd.get_dummies(df, columns=list(set(dummy_columns) - set(columns_to_drop)))\n",
    "    df_y = df[label_column].copy()\n",
    "    df_X = df.drop(columns=[label_column])\n",
    "\n",
    "    # stratify makes sure each class is equally represented precentage wise in the split\n",
    "    if stratify:\n",
    "        df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(df_X, df_y, test_size=test_perc, random_state=rand_state, stratify=df_y)\n",
    "    else:\n",
    "        df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(df_X, df_y, test_size=test_perc, random_state=rand_state)\n",
    "\n",
    "    # Note that I am shifiting all the labels so it works with all the models\n",
    "    if not (0 in list(df_y_train.unique())):\n",
    "        df_y_train = df_y_train - 1\n",
    "        df_y_test = df_y_test - 1\n",
    "\n",
    "    return df_X_train, df_y_train, df_X_test, df_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_sample(X: pd.DataFrame, y: pd.Series, label: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function downsamples all labels to the specified label\n",
    "    \n",
    "    Params\n",
    "    -------\n",
    "    X (pd.DataFrame): dataframe of training data\n",
    "    y (pd.Series): training labels\n",
    "    label (int): label to downsample to\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train (pd.DataFrame): down sampled training data\n",
    "    y_train (pd.Series): down sampled training labels\n",
    "    \"\"\"\n",
    "    counts = y.value_counts()\n",
    "    num_keep = len(y[y == label])\n",
    "    X_copy = X.copy()\n",
    "    y_copy = y.copy()\n",
    "    \n",
    "    for class_type in counts.index:\n",
    "        num_drop = counts[class_type] - num_keep\n",
    "        if num_drop > 0:\n",
    "            drop_indices = np.random.choice(y_copy[y_copy == class_type].index, num_drop, replace=False)\n",
    "            X_copy.drop(index=drop_indices, inplace=True)\n",
    "            y_copy.drop(index=drop_indices, inplace=True)\n",
    "    \n",
    "    return X_copy, y_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THE PATH FOR YOUR DATASET\n",
    "file = 'utah_county_accidents.csv'\n",
    "filepath = './data/' + file\n",
    "X_train, y_train, X_test, y_test = read_data_and_format(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the downsample on the data\n",
    "X_train, y_train = down_sample(X_train, y_train, DOWNSAMPLE_LABEL)\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick Note About Our Grid Search:\n",
    "It may be observed that we used different search sizes for each model. We originally made this decision due to time constraints and which parameters we felt were worth exploring. We recognize that this most likely biased our results. In future experiments, we would create more uniform grid searches so as to not bias our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Plain Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameters search\n",
    "# Actual Params\n",
    "param_grid = {\"criterion\": ['gini', 'entropy', 'log_loss'],\n",
    "              \"splitter\": ['best', 'random'],\n",
    "              \"min_samples_leaf\": [3, 6, 9, 12, 17, 20, 25, 28, 33],\n",
    "              \"max_features\": ['sqrt', 'log2']}\n",
    "\n",
    "# Make the trees\n",
    "model = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "model_gs = GridSearchCV(model, param_grid, refit='f1_macro', scoring=METRICS, n_jobs=-1)\n",
    "\n",
    "# Fit the trees\n",
    "model_gs.fit(X_train, y_train)\n",
    "\n",
    "# Print the best params and the report\n",
    "print(model_gs.best_params_, model_gs.best_score_, sep='\\n')\n",
    "y_pred = model_gs.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the score on the train set for comparison\n",
    "y_pred = model_gs.predict(X_train)\n",
    "print(classification_report(y_train, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for checking which classes got predicted and how frequently\n",
    "print(np.unique(y_pred, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = model_gs.best_estimator_.feature_importances_\n",
    "three_worst_feats = feat_importances.argsort() <= NUM_FEAT_IMPORTANCES - 1\n",
    "three_best_feats = feat_importances.argsort() > len(feat_importances) - (NUM_FEAT_IMPORTANCES + 1)\n",
    "\n",
    "print(\"The 3 best features are: \", np.array(X_train.columns)[three_best_feats])\n",
    "print(\"The 3 worst features are: \", np.array(X_train.columns)[three_worst_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a confusion matrix for visual analysis\n",
    "y_pred = model_gs.predict(X_test)\n",
    "ConfusionMatrixDisplay.from_predictions(y_pred, y_test, normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameters search\n",
    "# Actual Params\n",
    "param_grid = {\"n_estimators\":[50, 100, 150, 200, 250, 300],\n",
    "              \"criterion\": ['gini', 'entropy', 'log_loss'],\n",
    "              \"class_weight\": ['balanced', 'balanced_subsample'],\n",
    "              \"min_samples_leaf\":  [3, 6, 9, 12, 17, 20, 25, 28, 33],\n",
    "              \"max_features\": [round(len(X_train.columns)*perc) for perc in np.arange(.1, 1, .2)]}\n",
    "\n",
    "# Make the trees\n",
    "model = RandomForestClassifier(random_state=RANDOM_STATE, warm_start=False)\n",
    "model_gs = GridSearchCV(model, param_grid, refit='f1_macro', verbose=2, scoring=METRICS, n_jobs=-1)\n",
    "\n",
    "# Fit the trees\n",
    "model_gs.fit(X_train, y_train)\n",
    "\n",
    "# Print the best params and the report\n",
    "print(model_gs.best_params_, model_gs.best_score_, sep='\\n')\n",
    "y_pred = model_gs.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the score on the train set for comparison\n",
    "y_pred = model_gs.predict(X_train)\n",
    "print(classification_report(y_train, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for checking which classes got predicted and how frequently\n",
    "print(np.unique(y_pred, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = model_gs.best_estimator_.feature_importances_\n",
    "three_worst_feats = feat_importances.argsort() <= NUM_FEAT_IMPORTANCES - 1\n",
    "three_best_feats = feat_importances.argsort() > len(feat_importances) - (NUM_FEAT_IMPORTANCES + 1)\n",
    "\n",
    "print(\"The 3 best features are: \", np.array(X_train.columns)[three_best_feats])\n",
    "print(\"The 3 worst features are: \", np.array(X_train.columns)[three_worst_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a confusion matrix for visual analysis\n",
    "y_pred = model_gs.predict(X_test)\n",
    "ConfusionMatrixDisplay.from_predictions(y_pred, y_test, normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Gradient Boosted Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameters search\n",
    "# Actual Params\n",
    "param_grid = {\"loss\": ['log_loss', 'exponential'],\n",
    "              \"learning_rate\": [0.1, .2, .3, .4],\n",
    "              \"n_estimators\": [100, 200, 300],\n",
    "              \"criterion\": ['friedman_mse', 'squared_error'],\n",
    "              \"min_samples_leaf\":  [3, 6, 9, 12, 17, 20, 25, 28, 33],\n",
    "              \"max_features\": ['sqrt', 'log2'],\n",
    "              \"validation_fraction\": [0.1],\n",
    "             }\n",
    "\n",
    "# Make the trees\n",
    "model = GradientBoostingClassifier(random_state=RANDOM_STATE, warm_start=False)\n",
    "model_gs = GridSearchCV(model, param_grid, refit='f1_macro', verbose=2, scoring=METRICS, n_jobs=-1)\n",
    "\n",
    "# Fit the trees\n",
    "model_gs.fit(X_train, y_train)\n",
    "\n",
    "# Print the best params and the report\n",
    "print(model_gs.best_params_, model_gs.best_score_, sep='\\n')\n",
    "y_pred = model_gs.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the score on the train set for comparison\n",
    "y_pred = model_gs.predict(X_train)\n",
    "print(classification_report(y_train, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for checking which classes got predicted and how frequently\n",
    "print(np.unique(y_pred, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = model_gs.best_estimator_.feature_importances_\n",
    "three_worst_feats = feat_importances.argsort() <= NUM_FEAT_IMPORTANCES - 1\n",
    "three_best_feats = feat_importances.argsort() > len(feat_importances) - (NUM_FEAT_IMPORTANCES + 1)\n",
    "\n",
    "print(\"The 3 best features are: \", np.array(X_train.columns)[three_best_feats])\n",
    "print(\"The 3 worst features are: \", np.array(X_train.columns)[three_worst_feats])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train XGBoosted Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameters search\n",
    "# Actual Params\n",
    "param_grid = {\"gamma\":[0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1],\n",
    "              \"alpha\": [0.5, 3, 5, 10, 30, 50],\n",
    "              \"lambda\": [0, 5, 10, 25, 50, 100, 250, 500, 1000],\n",
    "              \"eta\": [.1, .2, .3, .4, .5, .6, .7, .8, .9, 1],\n",
    "              \"tree_method\": ['exact', 'approx', 'hist'],\n",
    "              }\n",
    "\n",
    "# Make the trees\n",
    "model = xgb.XGBClassifier(objective=\"multi:softmax\", random_state=RANDOM_STATE)\n",
    "model_gs = RandomizedSearchCV(model, param_grid, n_iter=2500, scoring=\"accuracy\", verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit the trees\n",
    "model_gs.fit(X_train, y_train)\n",
    "\n",
    "# Print the best params and the report\n",
    "print(model_gs.best_params_, model_gs.best_score_, sep='\\n')\n",
    "y_pred = model_gs.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the score on the train set for comparison\n",
    "y_pred = model_gs.predict(X_train)\n",
    "print(classification_report(y_train, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for checking which classes got predicted and how frequently\n",
    "print(np.unique(y_pred, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = model_gs.best_estimator_.feature_importances_\n",
    "three_worst_feats = feat_importances.argsort() <= NUM_FEAT_IMPORTANCES - 1\n",
    "three_best_feats = feat_importances.argsort() > len(feat_importances) - (NUM_FEAT_IMPORTANCES + 1)\n",
    "\n",
    "print(\"The 3 best features are: \", np.array(X_train.columns)[three_best_feats])\n",
    "print(\"The 3 worst features are: \", np.array(X_train.columns)[three_worst_feats])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acme",
   "language": "python",
   "name": "acme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
